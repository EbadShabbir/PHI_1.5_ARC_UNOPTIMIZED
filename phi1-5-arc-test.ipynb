{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install rouge_score\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom datasets import load_dataset\nimport torch, time, psutil, numpy as np, gc, re, json\nimport evaluate\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom rouge_score import rouge_scorer\n\n# Load model and tokenizer\nmodel_name = \"microsoft/phi-1_5\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.padding_side = \"left\"\ntokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name, trust_remote_code=True,\n    torch_dtype=torch.float16, device_map=\"auto\"\n)\ndevice = model.device\ntorch.cuda.empty_cache()\n\n# Load ARC-Challenge dataset\ndataset = load_dataset(\"allenai/ai2_arc\", \"ARC-Challenge\", split=\"test[:500]\")\n\nsmoother = SmoothingFunction().method4\nscorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n\ndef evaluate_phi15_on_arc(dataset, batch_size=4, max_new_tokens=50):\n    accuracy_metric = evaluate.load(\"accuracy\")\n    f1_metric = evaluate.load(\"f1\")\n    process = psutil.Process()\n    results = {\n        \"accuracies\": [], \"f1_scores\": [], \"latencies\": [], \"tokens_per_sec\": [], \"memory_usage\": [],\n        \"retrieval_latencies\": [], \"query_times\": [], \"memory_reductions\": [],\n        \"bleu_scores\": [], \"rouge1_scores\": [], \"rougeL_scores\": [],\n        \"knowledge_retentions\": [], \"accuracy_drops\": [], \"compression_ratios\": []\n    }\n\n    initial_memory = process.memory_info().rss / 1024**3\n\n    for i in range(0, len(dataset), batch_size):\n        prompts, correct_answers, references = [], [], []\n\n        for idx in range(i, min(i + batch_size, len(dataset))):\n            item = dataset[idx]\n            question = item[\"question\"]\n            choices = item[\"choices\"]\n            labels = choices[\"label\"]\n            texts = choices[\"text\"]\n            answer_key = item[\"answerKey\"]\n\n            prompt = f\"Question: {question}\\n\"\n            for label, choice in zip(labels, texts):\n                prompt += f\"{label}: {choice}\\n\"\n            prompt += \"Answer:\"\n            explanation = f\"{question} \" + \" \".join(texts)\n\n            prompts.append(prompt)\n            correct_answers.append(answer_key)\n            references.append(explanation)\n\n        start_retrieval = time.time()\n        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n        retrieval_latency = time.time() - start_retrieval\n        results[\"retrieval_latencies\"].append(retrieval_latency)\n\n        start = time.time()\n        with torch.no_grad():\n            outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.eos_token_id)\n        latency = time.time() - start\n        results[\"latencies\"].append(latency)\n\n        generated_tokens = sum(len(out) - len(inp) for out, inp in zip(outputs, inputs['input_ids']))\n        results[\"tokens_per_sec\"].append(generated_tokens / latency if latency > 0 else 0)\n        results[\"query_times\"].append(time.time() - start)\n\n        final_memory = process.memory_info().rss / 1024**3\n        results[\"memory_usage\"].append(final_memory)\n        results[\"memory_reductions\"].append(max(0, initial_memory - final_memory))\n\n        generated_texts = [tokenizer.decode(out, skip_special_tokens=True) for out in outputs]\n        pred_answers = []\n        for text in generated_texts:\n            match = re.search(r\"\\b([A-E])\\b\", text.split(\"Answer\")[-1])\n            pred_answers.append(match.group(1).strip().upper() if match else \"\")\n\n        try:\n            accuracy = accuracy_metric.compute(predictions=pred_answers, references=correct_answers)[\"accuracy\"]\n            f1 = f1_metric.compute(predictions=pred_answers, references=correct_answers, average=\"macro\")[\"f1\"]\n        except:\n            accuracy, f1 = 0, 0\n        results[\"accuracies\"].append(accuracy)\n        results[\"f1_scores\"].append(f1)\n\n        for gen, ref in zip(generated_texts, references):\n            gen_words = gen.split()\n            ref_words = ref.split()\n\n            try:\n                bleu = sentence_bleu([ref_words], gen_words, weights=(0.5, 0.5, 0.0, 0.0), smoothing_function=smoother)\n                results[\"bleu_scores\"].append(bleu)\n            except:\n                results[\"bleu_scores\"].append(0)\n\n            try:\n                rouge = scorer.score(ref, gen)\n                results[\"rouge1_scores\"].append(rouge['rouge1'].fmeasure)\n                results[\"rougeL_scores\"].append(rouge['rougeL'].fmeasure)\n                results[\"knowledge_retentions\"].append(rouge['rougeL'].fmeasure)\n                results[\"accuracy_drops\"].append(1 - rouge['rouge1'].fmeasure)\n            except:\n                results[\"rouge1_scores\"].append(0)\n                results[\"rougeL_scores\"].append(0)\n                results[\"knowledge_retentions\"].append(0)\n                results[\"accuracy_drops\"].append(0)\n\n            try:\n                input_tokens = len(inputs['input_ids'][0])\n                output_tokens = len(outputs[0])\n                results[\"compression_ratios\"].append(input_tokens / output_tokens if output_tokens > 0 else 1)\n            except:\n                results[\"compression_ratios\"].append(1)\n\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    # Print full metric summary\n    summary = {k: np.mean(v) if v else 0 for k, v in results.items()}\n    print(f\"Avg latency: {summary['latencies']:.3f} sec\")\n    print(f\"Tokens per sec: {summary['tokens_per_sec']:.2f}\")\n    print(f\"BLEU Score: {summary['bleu_scores']:.3f}\")\n    print(f\"ROUGE-1 Score: {summary['rouge1_scores']:.3f}\")\n    print(f\"ROUGE-L Score: {summary['rougeL_scores']:.3f}\")\n    print(f\"Memory usage (GB): {summary['memory_usage']:.3f}\")\n    print(f\"Retrieval Latency (sec): {summary['retrieval_latencies']:.3f}\")\n    print(f\"F1 Score: {summary['f1_scores']:.3f}\")\n    print(f\"Knowledge Retention: {summary['knowledge_retentions']:.3f}\")\n    print(f\"Memory Reduction (GB): {summary['memory_reductions']:.2f}\")\n    print(f\"Query Processing Time (sec): {summary['query_times']:.3f}\")\n    print(f\"Accuracy Drop: {summary['accuracy_drops']:.3f}\")\n    print(f\"Compression Ratio: {summary['compression_ratios']:.2f}\")\n    print(f\"Accuracy: {summary['accuracies']:.3f}\")\n\n    with open(\"/kaggle/working/arc_phi15_metrics.json\", \"w\") as f:\n        json.dump(summary, f, indent=2)\n\n    return summary\n\n# Run evaluation\narc_metrics = evaluate_phi15_on_arc(dataset, batch_size=4, max_new_tokens=50)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T07:32:14.034716Z","iopub.execute_input":"2025-04-19T07:32:14.035454Z","iopub.status.idle":"2025-04-19T07:36:46.591373Z","shell.execute_reply.started":"2025-04-19T07:32:14.035420Z","shell.execute_reply":"2025-04-19T07:36:46.590612Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge_score) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rouge_score) (2024.2.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=71b63d43b649b7d3f10f0bc1cb32d7e29f9d7acd2f8dde6de8981f2839a01db3\n  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc4d47e36bd54b919bb58a2755f7be66"}},"metadata":{}},{"name":"stdout","text":"Avg latency: 1.724 sec\nTokens per sec: 116.08\nBLEU Score: 0.446\nROUGE-1 Score: 0.639\nROUGE-L Score: 0.639\nMemory usage (GB): 2.057\nRetrieval Latency (sec): 0.002\nF1 Score: 0.000\nKnowledge Retention: 0.639\nMemory Reduction (GB): 0.00\nQuery Processing Time (sec): 1.724\nAccuracy Drop: 0.361\nCompression Ratio: 0.64\nAccuracy: 0.000\n","output_type":"stream"}],"execution_count":8}]}